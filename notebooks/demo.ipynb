{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from pypesq import pesq\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import display, Audio\n",
    "from src import CustomUNet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import graphviz\n",
    "from torchview import draw_graph\n",
    "from data import get_dataloader, dataloader_sampler, visualize_spectrogram\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and visualizing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dir = os.path.abspath(\"../data/speech_files\")\n",
    "speech_files = glob.glob(os.path.join(speech_dir, \"*.wav\"))\n",
    "train_dataloader, val_dataloader = get_dataloader(\n",
    "    speech_dir, split_ratio=0.2, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_sampler(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_spectrogram(train_dataloader, num_samples=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.set_jupyter_format(\"svg\")\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomUNet(input_shape=(1, 256, 512), num_classes=1)\n",
    "\n",
    "model_graph = draw_graph(model, input_size=(batch_size, 1, 256, 512), device=\"meta\")\n",
    "model_graph.fill_visual_graph()\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = CustomUNet(input_shape=(1, 256, 512), num_classes=1)\n",
    "model.to(device)\n",
    "\n",
    "# Define custom loss function\n",
    "class SignalEnhancementLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        mae = torch.abs(y_true - y_pred)\n",
    "        speech_loss = 2 * torch.abs(y_true**2 - y_pred * y_true)\n",
    "        loss = torch.mean(mae, dim=-1) + torch.mean(speech_loss, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# criterion = SignalEnhancementLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 30\n",
    "\n",
    "# Create a SummaryWriter to log training and validation losses to TensorBoard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "# Initialize best validation loss to infinity\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_bar_desc = f\"{Fore.LIGHTMAGENTA_EX}Epoch {epoch+1} >>> Train Loss: {{train_loss:.4f}}{Style.RESET_ALL}\"\n",
    "    val_bar_desc = f\"{Fore.LIGHTCYAN_EX}Epoch {epoch+1} >>> Val Loss: {{val_loss:.4f}}{Style.RESET_ALL}\"\n",
    "    train_bar = tqdm(train_dataloader, ncols=80, desc=train_bar_desc)\n",
    "    val_bar = tqdm(val_dataloader, ncols=80, desc=val_bar_desc)\n",
    "    for i, data in enumerate(train_bar, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        train_loss = running_loss / (i + 1)\n",
    "        train_bar.set_description_str(train_bar_desc.format(train_loss=train_loss))\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    writer.add_scalar(\"Train Loss\", train_loss, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_val_loss = 0.0\n",
    "        for i, data in enumerate(val_bar, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            val_loss = running_val_loss / (i + 1)\n",
    "            val_bar.set_description_str(val_bar_desc.format(val_loss=val_loss))\n",
    "\n",
    "        val_loss = running_val_loss / len(val_dataloader)\n",
    "        writer.add_scalar(\"Val Loss\", val_loss, epoch)\n",
    "        tqdm.write(\n",
    "            \"\"\n",
    "        )  # Print a new line after the TQDM training loss bar fills for each epoch\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = CustomUNet(input_shape=(1, 128, 256), num_classes=1)\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Load the weights from the .pth file\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_dataloader = get_dataloader(\n",
    "    speech_dir, split_ratio=0.05, batch_size=1, test=True\n",
    ")\n",
    "num_files = test_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "sr = 16000\n",
    "speech_length_pix_sec = 27e-3\n",
    "total_length = 3.6\n",
    "trim_length = 56800\n",
    "n_fft = 510\n",
    "frame_length = 510\n",
    "frame_step = 110\n",
    "\n",
    "# Define the L1 loss function\n",
    "mae = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize some arrays to store results\n",
    "pesq_with_noise = np.zeros(num_files)\n",
    "pesq_denoised = np.zeros(num_files)\n",
    "wav_clean_array = np.zeros((num_files, trim_length))\n",
    "wav_corrupt_array = np.zeros((num_files, trim_length))\n",
    "wav_correct_array = np.zeros((num_files, trim_length))\n",
    "spec_clean_array = np.zeros((num_files, 256, 512))\n",
    "spec_corrupt_array = np.zeros((num_files, 256, 512))\n",
    "spec_correct_array = np.zeros((num_files, 256, 512))\n",
    "loss_with_noise = np.zeros(num_files)\n",
    "loss_denoised = np.zeros(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    # Loop over the test data\n",
    "    for ind, (corr, clean) in enumerate(test_dataloader):\n",
    "        # Squeeze the batch dimension\n",
    "        corr = corr.squeeze(0)\n",
    "        clean = clean.squeeze(0)\n",
    "\n",
    "        # Convert corrupted and clean spectrograms to waveforms\n",
    "        corr_wav = torch.istft(\n",
    "            corr[0, :, :],\n",
    "            n_fft=n_fft,\n",
    "            hop_length=frame_step,\n",
    "            win_length=frame_length,\n",
    "            onesided=True,\n",
    "            length=trim_length,\n",
    "        )\n",
    "        clean_wav = torch.istft(\n",
    "            clean[0, :, :],\n",
    "            n_fft=n_fft,\n",
    "            hop_length=frame_step,\n",
    "            win_length=frame_length,\n",
    "            onesided=True,\n",
    "            length=trim_length,\n",
    "        )\n",
    "\n",
    "        # Convert corrupted spectrogram to numpy array and compute its magnitude\n",
    "        corr = corr.detach().cpu().numpy()\n",
    "        corr_amp = np.abs(corr)\n",
    "\n",
    "        # Apply the model to the corrupted spectrogram\n",
    "        corrected_amp = model(\n",
    "            torch.from_numpy(np.expand_dims(corr_amp, axis=0)).to(\"cuda\")\n",
    "        )\n",
    "        corrected_spec = corrected_amp.detach().cpu().numpy() * np.exp(\n",
    "            1j * np.angle(np.expand_dims(corr, 0))\n",
    "        )\n",
    "        corrected_spec = torch.from_numpy(corrected_spec).to(\"cuda\")\n",
    "\n",
    "        # Convert the corrected spectrogram to a waveform\n",
    "        corrected_wav = torch.istft(\n",
    "            corrected_spec[0, 0, :, :],\n",
    "            n_fft=n_fft,\n",
    "            hop_length=frame_step,\n",
    "            win_length=frame_length,\n",
    "            onesided=True,\n",
    "            length=trim_length,\n",
    "        )\n",
    "\n",
    "        # Compute PESQ scores for the corrupted and corrected waveforms\n",
    "        pesq_with_noise[ind] = pesq(\n",
    "            clean_wav.detach().cpu().numpy(), corr_wav.detach().cpu().numpy(), sr\n",
    "        )\n",
    "        pesq_denoised[ind] = pesq(\n",
    "            clean_wav.detach().cpu().numpy(), corrected_wav.detach().cpu().numpy(), sr\n",
    "        )\n",
    "\n",
    "        # Store the waveforms and spectrograms in the arrays\n",
    "        wav_clean_array[ind] = clean_wav\n",
    "        wav_corrupt_array[ind] = corr_wav\n",
    "        wav_correct_array[ind] = corrected_wav.detach().cpu().numpy()\n",
    "        spec_clean_array[ind] = np.abs(clean[0, :, :])\n",
    "        spec_corrupt_array[ind] = np.abs(corr[0, :, :])\n",
    "        spec_correct_array[ind] = corrected_amp[0, 0, :, :].detach().cpu().numpy()\n",
    "\n",
    "        # Compute the L1 loss between the clean spectrogram and the corrupted spectrogram\n",
    "        loss_with_noise[ind] = mae(\n",
    "            torch.from_numpy(np.abs(clean[0, :, :].numpy())).to(\"cuda\"),\n",
    "            torch.from_numpy(corr_amp).squeeze(0).to(\"cuda\"),\n",
    "        ).detach().cpu().numpy()\n",
    "\n",
    "        # Compute the L1 loss between the clean spectrogram and the corrected spectrogram\n",
    "        loss_denoised[ind] = mae(\n",
    "            torch.from_numpy(np.abs(clean[0, :, :].numpy())).to(\"cuda\"),\n",
    "            corrected_amp[0, 0, :, :].squeeze(0),\n",
    "        ).detach().cpu().numpy()\n",
    "\n",
    "# Compute the difference in PESQ scores between the corrected and corrupted waveforms\n",
    "pesq_diff = pesq_denoised - pesq_with_noise\n",
    "\n",
    "# Print the mean PESQ scores for the corrupted and corrected waveforms, and their difference\n",
    "print(f\"{Fore.CYAN}PESQ with Noise:{Fore.RESET} {np.mean(pesq_with_noise):.2f}\")\n",
    "print(f\"{Fore.CYAN}PESQ Denoised:{Fore.RESET} {np.mean(pesq_denoised):.2f}\")\n",
    "print(f\"{Fore.CYAN}PESQ Difference:{Fore.RESET} {pesq_diff.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the difference in PESQ scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(pesq_diff, bins=20)\n",
    "plt.xlabel(\"PESQ Difference\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"PESQ Difference Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the waveforms to .wav files\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "ind = np.where(pesq_diff == pesq_diff.max())[0][0]\n",
    "sf.write(\"results/clean.wav\", wav_clean_array[ind], samplerate=sr)\n",
    "sf.write(\"results/corrupt.wav\", wav_corrupt_array[ind], samplerate=sr)\n",
    "sf.write(\"results/corrected.wav\", wav_correct_array[ind], samplerate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to the waveforms\n",
    "print(f\"{Fore.LIGHTMAGENTA_EX}Clean Audio >> {Fore.RESET}\")\n",
    "display(Audio(\"results/clean.wav\"))\n",
    "print(f\"{Fore.LIGHTCYAN_EX}Corrupted Audio >> {Fore.RESET}\")\n",
    "display(Audio(\"results/corrupt.wav\"))\n",
    "print(f\"{Fore.LIGHTRED_EX}Corrected Audio >> {Fore.RESET}\")\n",
    "display(Audio(\"results/corrected.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spectrograms\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.pcolormesh(np.log(spec_clean_array[ind]))\n",
    "plt.title(\"Clean Spectrogram\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.pcolormesh(np.log(spec_corrupt_array[ind]))\n",
    "plt.title(\"Corrupted Spectrogram\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.pcolormesh(np.log(spec_correct_array[ind]))\n",
    "plt.title(\"Corrected Spectrogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss histograms\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(loss_with_noise, bins=20)\n",
    "plt.xlabel(\"L1 Loss\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"L1 Loss Histogram (Corrupted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 50\n",
    "# Apply the model to the corrupted spectrogram\n",
    "corrected_amp = model(torch.from_numpy(np.expand_dims(corr_amp, axis=0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on an unseen noisy audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = CustomUNet(input_shape=(1, 128, 256), num_classes=1)\n",
    "model.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Load the weights from the .pth file\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "files_list = glob.glob(\"samples/*.wav\")\n",
    "sample_dataloader = get_dataloader(\n",
    "    speech_files=files_list,\n",
    "    split_ratio=None,\n",
    "    batch_size=1,\n",
    "    test=True,\n",
    "    shuffle=False,\n",
    "    sample_only=True,\n",
    ")\n",
    "num_files = sample_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "sr = 16000\n",
    "speech_length_pix_sec = 27e-3\n",
    "total_length = 3.6\n",
    "trim_length = 56800\n",
    "n_fft = 510\n",
    "frame_length = 510\n",
    "frame_step = 110\n",
    "\n",
    "# Define the L1 loss function\n",
    "mae = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize some arrays to store results\n",
    "wav_corrupt_array = np.zeros((num_files, trim_length))\n",
    "wav_correct_array = np.zeros((num_files, trim_length))\n",
    "spec_corrupt_array = np.zeros((num_files, 256, 512))\n",
    "spec_correct_array = np.zeros((num_files, 256, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "\n",
    "# Disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    # Loop over the test data\n",
    "    for ind, corr in enumerate(sample_dataloader):\n",
    "\n",
    "        # Convert corrupted and clean spectrograms to waveforms\n",
    "        corr_wav = torch.istft(\n",
    "            corr[0, :, :],\n",
    "            n_fft=n_fft,\n",
    "            hop_length=frame_step,\n",
    "            win_length=frame_length,\n",
    "            onesided=True,\n",
    "            length=trim_length,\n",
    "        )\n",
    "\n",
    "        # Convert corrupted spectrogram to numpy array and compute its magnitude\n",
    "        corr = corr.detach().cpu().numpy()\n",
    "        corr_amp = np.abs(corr)\n",
    "\n",
    "        # Apply the model to the corrupted spectrogram\n",
    "        corrected_amp = model(torch.from_numpy(np.expand_dims(corr_amp, axis=0)))\n",
    "        corrected_spec = corrected_amp * np.exp(1j * np.angle(np.expand_dims(corr, 0)))\n",
    "\n",
    "        # Convert the corrected spectrogram to a waveform\n",
    "        corrected_wav = torch.istft(\n",
    "            corrected_spec[0, 0, :, :],\n",
    "            n_fft=n_fft,\n",
    "            hop_length=frame_step,\n",
    "            win_length=frame_length,\n",
    "            onesided=True,\n",
    "            length=trim_length,\n",
    "        )\n",
    "\n",
    "        # Store the waveforms and spectrograms in the arrays\n",
    "        wav_corrupt_array[ind] = corr_wav\n",
    "        wav_correct_array[ind] = corrected_wav\n",
    "        spec_corrupt_array[ind] = np.abs(corr[0, :, :])\n",
    "        spec_correct_array[ind] = corrected_amp[0, 0, :, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for ind in range(num_files):\n",
    "        corrupt_file = os.path.join(temp_dir, f\"{ind}_corrupt.wav\")\n",
    "        corrected_file = os.path.join(temp_dir, f\"{ind}_correct.wav\")\n",
    "        sf.write(\n",
    "            corrupt_file,\n",
    "            wav_corrupt_array[ind],\n",
    "            samplerate=sr,\n",
    "        )\n",
    "        sf.write(\n",
    "            corrected_file,\n",
    "            wav_correct_array[ind],\n",
    "            samplerate=sr,\n",
    "        )\n",
    "        print(f\"{Fore.LIGHTCYAN_EX}Corrupted Audio {ind + 1} >> {Fore.RESET}\")\n",
    "        display(Audio(corrupt_file))\n",
    "        print(f\"{Fore.LIGHTRED_EX}Corrected Audio {ind + 1} >> {Fore.RESET}\")\n",
    "        display(Audio(corrected_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spectrograms\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(np.log(spec_corrupt_array[ind]))\n",
    "plt.title(\"Corrupted Spectrogram\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(np.log(spec_correct_array[ind]))\n",
    "plt.title(\"Corrected Spectrogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-suppressor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
